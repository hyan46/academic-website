
@article{fangMultiSensor2020,
  title = {Multi-{{Sensor Prognostics Modeling}} for {{Applications}} with {{Highly Incomplete Signals}}},
  author = {Fang, Xiaolei and Yan, Hao and Gebraeel, Nagi and Paynabar, Kamran},
  year = {2020},
  month = jul,
  pages = {1--30},
  issn = {2472-5854, 2472-5862},
  doi = {10.1080/24725854.2020.1789779},
  abstract = {Multi-stream degradation signals have been widely used to predict the residual useful lifetime of partially degraded systems. To achieve this goal, most of the existing prognostics models assume that degradation signals are complete, i.e., they are observed continuously and frequently at regular time grids. In reality, however, degradation signals are often (highly) incomplete, i.e., containing missing and corrupt observations. Such signal incompleteness poses a significant challenge for the parameter estimation of prognostics models. To address this challenge, this article proposes a prognostics methodology that is capable of using highly incomplete multi-stream degradation signals to predict the residual useful lifetime of partially degraded systems. The method first employs multivariate functional principal components analysis to fuse multi-stream signals. Next, the fused features are regressed against time-to-failure using (log)-location-scale regression. To estimate the fused features using incomplete multi-stream degradation signals, we develop two computationally efficient algorithms: subspace detection and signal recovery. The performance of the proposed prognostics methodology is evaluated using simulated datasets and a degradation dataset of aircraft turbofan engines from the NASA repository.},
  copyright = {All rights reserved},
  file = {/Users/hyan46/Dropbox (ASU)/ZoteroData/Fang et al_2020_Multi-Sensor Prognostics Modeling for Applications with Highly Incomplete2.pdf},
  journal = {IISE Transactions},
  keywords = {Matrix Decomposition,Prognostics,Signal},
  language = {en}
}

@article{gahrooeiMultiple2020,
  title = {Multiple {{Tensor}}-on-{{Tensor Regression}}: {{An Approach}} for {{Modeling Processes With Heterogeneous Sources}} of {{Data}}},
  author = {Gahrooei, Mostafa Reisi and Yan, Hao and Paynabar, Kamran and Shi, Jianjun},
  year = {2020},
  publisher = {{American Statistical Association}},
  issn = {15372723},
  doi = {10.1080/00401706.2019.1708463},
  abstract = {In recent years, measurement or collection of heterogeneous sets of data such as those containing scalars, waveform signals, images, and even structured point clouds, has become more common. Statistical models based on such heterogeneous sets of data that represent the behavior of an underlying system can be used in the monitoring, control, and optimization of the system. Unfortunately, available methods mainly focus on the scalars and profiles and do not provide a general framework for integrating different sources of data to construct a model. This article addresses the problem of estimating a process output, measured by a scalar, curve, image, or structured point cloud by a set of heterogeneous process variables such as scalar process setting, profile sensor readings, and images. We introduce a general multiple tensor-on-tensor regression approach in which each set of input data (predictor) and output measurements are represented by tensors. We formulate a linear regression model between the input and output tensors and estimate the parameters by minimizing a least square loss function. To avoid overfitting and reduce the number of parameters to be estimated, we decompose the model parameters using several basis matrices that span the input and output spaces, and provide efficient optimization algorithms for learning the basis and coefficients. Through several simulation and case studies, we evaluate the performance of the proposed method. The results reveal the advantage of the proposed method over some benchmarks in the literature in terms of the mean square prediction error. Supplementary materials for this article are available online.},
  annotation = {\_eprint: 1803.00138},
  file = {/Users/hyan46/Dropbox (ASU)/ZoteroData/Gahrooei et al_2020_Multiple Tensor-on-Tensor Regression3.pdf},
  journal = {Technometrics},
  keywords = {Classification,Manufacturing,Tensor}
}

@article{haoyanDeep2021,
  title = {Deep {{Multistage Multi}}-{{Task Learning}} for {{Quality Prediction}} and {{Diagnostics}} of {{Multistage Manufacturing Systems}}},
  author = {{Hao Yan} and {Nurretin D. Sergin} and {William A. Brenneman} and {Stephen J. Lange} and {Shan Ba}},
  year = {2021},
  pages = {1--27},
  doi = {10.1080/00224065.2021.1903822},
  abstract = {In multistage manufacturing systems, modeling multiple quality indices based on the process sensing variables is important. However, the classic modeling technique predicts each quality variable one at a time, which fails to consider the correlation within or between stages. We propose a deep multistage multi-task learning framework to jointly predict all output sensing variables in a unified end-to-end learning framework according to the sequential system architecture in the MMS. Our numerical studies and real case study have shown that the new model has a superior performance compared to many benchmark methods as well as great interpretability through developed variable selection techniques.},
  copyright = {All rights reserved},
  journal = {Journal of Quality Technology},
  keywords = {Deep Learning,Manufacturing,Multi-task Learning,Regression,Signal}
}

@inproceedings{huangEdge2021,
  title = {Edge {{Computing Accelerated Defect Classification Based}} on {{Deep Convolutional Neural Network With Application}} in {{Rolling Image Inspection}}},
  booktitle = {{{ASME}} 2020 15th {{International Manufacturing Science}} and {{Engineering Conference}}},
  author = {Huang, Jiayu and Sergin, Nurretin and Dua, Akshay and Tavakoli, Erfan Bank and Yan, Hao and Ren, Fengbo and Ju, Feng},
  year = {2021},
  month = jan,
  publisher = {{American Society of Mechanical Engineers Digital Collection}},
  doi = {10.1115/MSEC2020-8261},
  abstract = {This paper develops a unified framework for training and deploying deep neural networks on the edge computing framework for image defect detection and classification. In the proposed framework, we combine the transfer learning and data augmentation with the improved accuracy given the small sample size. We further implement the edge computing framework to satisfy the real-time computational requirement. After the implement of the proposed model into a rolling manufacturing system, we conclude that deep learning approaches can perform around 30\textendash 40\% better than some traditional machine learning algorithms such as random forest, decision tree, and SVM in terms of prediction accuracy. Furthermore, by deploying the CNNs in the edge computing framework, we can significantly reduce the computational time and satisfy the real-time computational requirement in the high-speed rolling and inspection system. Finally, the saliency map and embedding layer visualization techniques are used for a better understanding of proposed deep learning models.},
  copyright = {All rights reserved},
  keywords = {Classification,Deep Learning,Image,Manufacturing},
  language = {en}
}

@article{kangPerformance2020,
  title = {Performance {{Evaluation}} of {{Production Systems Using Real}}-{{Time Machine Degradation Signals}}},
  author = {Kang, Yunyi and Yan, Hao and Ju, Feng},
  year = {2020},
  month = jan,
  volume = {17},
  pages = {273--283},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {15583783},
  doi = {10.1109/TASE.2019.2920874},
  abstract = {A machine's degradation status directly influences the operational performance of the production system, such as productivity and product quality. For example, machines associated with different health states may have different remaining life before failure, thus impacting the system throughput. Therefore, it is critical to analyze the coupling between the overall system performance and the machine degradation to better production decision-making, such as maintenance and product dispatch decisions. In this paper, we propose a novel model to evaluate the production performance of a two-machine-and-one-buffer line, given the real-time machine degradation signals. Specifically, a phase-type distribution-based continuous-time Markov chain model is formulated to estimate the system throughput by utilizing the remaining life prediction from the degradation signals. A case study is provided to demonstrate the applicability and effectiveness of the proposed method.Note to Practitioners - Machine degradation is commonly observed in many industries, such as automotive, semiconductor, and food production, which gradually deteriorates the machine conditions in different operating processes and affects the production system performance. In practice, sensors are largely deployed on the factory floor to monitor the machine's operating condition. However, a gap still exists between machine operating conditions and system performance. In this paper, we develop an analytical model to predict the machine remaining lifetime and estimate the system performance of a small scale production system, using the machine degradation signals from sensors. Furthermore, a Bayesian updating scheme is provided, which enables online evaluation by utilizing the real-time signals. Such a method provides an effective tool for production engineers to analyze the real-time system performance, and further conduct system improvements and control.},
  file = {/Users/hyan46/Dropbox (ASU)/ZoteroData/Kang et al_2020_Performance Evaluation of Production Systems Using Real-Time Machine2.pdf},
  journal = {IEEE Transactions on Automation Science and Engineering},
  keywords = {Bayesian,Control,Manufacturing},
  number = {1}
}

@inproceedings{kangRealtime2018,
  title = {Real-Time {{Production Performance Analysis Using Machine Degradation Signals}}: {{A Two}}-{{Machine Case}}},
  booktitle = {2018 {{IEEE}} 14th {{International Conference}} on {{Automation Science}} and {{Engineering}} ({{CASE}})},
  author = {Kang, Yunyi and Yan, Hao and Ju, Feng},
  year = {2018},
  month = dec,
  volume = {2018-Augus},
  pages = {1501--1506},
  publisher = {{IEEE Computer Society}},
  issn = {21618089},
  doi = {10.1109/COASE.2018.8560385},
  abstract = {Ahstract- Machine degradation has significant impact on the production system performance. Its variation might lead to large deviation from the steady state performance of the whole system. In this work, we build up a model to estimate the long-term production performance of the two-machine-and-one-buffer production systems, given the real-time machine degradation signals. A phase-type distribution is generated to mimic the remaining life distribution of each machine given the degradation signal. Then a continuous Markovian model is formulated to predict longterm system throughput rate for a two-machine-and-one-buffer system. With the fluctuation of machine degradation signals, such a model can effectively estimate the expected system performance in real-time.},
  file = {/Users/hyan46/Dropbox (ASU)/ZoteroData/Kang et al_2018_Real-time Production Performance Analysis Using Machine Degradation Signals2.pdf},
  isbn = {978-1-5386-3593-3},
  keywords = {Bayesian,Control,Manufacturing,Prognostics}
}

@article{lahotiImage2021,
  title = {Image Decomposition-Based Sparse Extreme Pixel-Level Feature Detection Model with Application to Medical Images},
  author = {Lahoti, Geet and Chen, Jialei and Yue, Xiaowei and Yan, Hao and Ranjan, Chitta and Qian, Zhen and Zhang, Chuck and Wang, Ben},
  year = {2021},
  month = apr,
  volume = {0},
  pages = {1--17},
  publisher = {{Taylor \& Francis}},
  issn = {2472-5579},
  doi = {10.1080/24725579.2021.1910599},
  abstract = {Pixel-level feature detection from images is an essential but challenging task encountered in domains such as detecting defects in manufacturing systems and detecting tumors in medical imaging. Often, the real image contains multiple feature types. The types with higher pixel intensities are termed as positive (extreme) features and the ones with lower pixel intensities as negative (extreme) features. For example, when planning a medical treatment, it is important to identify, (a) calcification (a pathological feature which can result in a post-surgical complications) as positive features, and (b) soft tissues (organ morphology, knowledge of which can support pre-surgical planning) as negative features, from a preoperative computed tomography image of the human heart. However, this is not an easy task because (a) conventional segmentation techniques require manual intervention and post-processing, and (b) existing automatic approaches do not distinguish positive features from negative. In this work, we propose a novel, automatic image decomposition-based sparse extreme pixel-level feature detection model to decompose an image into mean and extreme features. To estimate model parameters, a high-dimensional least squares regression with regularization and constraints is utilized. An efficient algorithm based on the alternating direction method of multipliers and the proximal gradient method is developed to solve the large-scale optimization problem. The effectiveness of the proposed model is demonstrated using synthetic tests and a real-world case study, where the model exhibits superior performance over existing methods.},
  annotation = {\_eprint: https://doi.org/10.1080/24725579.2021.1910599},
  copyright = {CC0 1.0 Universal Public Domain Dedication},
  file = {/Users/hyan46/Zotero/storage/5Q6MMSAM/24725579.2021.html},
  journal = {IISE Transactions on Healthcare Systems Engineering},
  keywords = {Health,High-dimensional,Segmentation},
  number = {0}
}

@inproceedings{liTensor2020,
  title = {Tensor {{Completion}} for {{Weakly}}-Dependent {{Data}} on {{Graph}} for {{Metro Passenger Flow Prediction}}},
  booktitle = {Thirty-{{Fourth AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Li, Ziyue and Sergin, Nurettin Dorukhan and Yan, Hao and Zhang, Chen and Tsung, Fugee},
  year = {2020},
  month = dec,
  doi = {10.1609/aaai.v34i04.5915},
  abstract = {Low-rank tensor decomposition and completion have attracted significant interest from academia given the ubiquity of tensor data. However, the low-rank structure is a global property, which will not be fulfilled when the data presents complex and weak dependencies given specific graph structures. One particular application that motivates this study is the spatiotemporal data analysis. As shown in the preliminary study, weakly dependencies can worsen the low-rank tensor completion performance. In this paper, we propose a novel low-rank CANDECOMP / PARAFAC (CP) tensor decomposition and completion framework by introducing the \$L\_\{1\}\$-norm penalty and Graph Laplacian penalty to model the weakly dependency on graph. We further propose an efficient optimization algorithm based on the Block Coordinate Descent for efficient estimation. A case study based on the metro passenger flow data in Hong Kong is conducted to demonstrate improved performance over the regular tensor completion methods.},
  annotation = {\_eprint: 1912.05693},
  file = {/Users/hyan46/Dropbox (ASU)/ZoteroData/Li et al_2020_Tensor Completion for Weakly-dependent Data on Graph for Metro Passenger Flow3.pdf},
  keywords = {Forecasting,Tensor,Time Series,Traffic}
}

@article{mesnilFast2016,
  title = {Fast Wavenumber Measurement for Accurate and Automatic Location and Quantification of Defect in Composite},
  author = {Mesnil, Olivier and Yan, Hao and Ruzzene, Massimo and Paynabar, Kamran and Shi, Jianjun},
  year = {2016},
  month = mar,
  volume = {15},
  pages = {223--234},
  publisher = {{SAGE Publications Ltd}},
  issn = {17413168},
  doi = {10.1177/1475921716636375},
  abstract = {As the use of and dependence on composite materials is increasing in all industries, there is a strong need for reliable, accurate, and fast techniques for damage detection and quantification in composite plate-like structures. Among the various nondestructive evaluation and structural health monitoring techniques, many rely on Lamb wave long-range propagation. A wavenumber quantification technique called frequency domain instantaneous wavenumber has been previously proven to be an efficient technique to estimate in-plane (i.e. position) and out-of-plane (i.e. depth) coordinates of defects in composites. This paper further develops this technique by (1) reducing the acquisition time by one order of magnitude while improving the quality of detection, (2) implementing an automatic feature extraction process to automatically assess the geometry and obtain information which can be directly used for decision making, and (3) quantifying the cumulative error of the whole process.},
  file = {/Users/hyan46/Dropbox (ASU)/ZoteroData/Mesnil et al_2016_Fast wavenumber measurement for accurate and automatic location and2.pdf},
  journal = {Structural Health Monitoring},
  keywords = {Anomaly Detection,Material,Physics},
  number = {2}
}

@inproceedings{mesnilFrequency2014,
  title = {Frequency {{Domain Instantaneous Wavenumber Estimation}} for {{Damage Quantification}} in {{Layered Plate Structures}}},
  booktitle = {{{EWSHM}} - 7th {{European Workshop}} on {{Structural Health Monitoring}}},
  author = {Mesnil, Olivier and Yan, Hao and Ruzzene, Massimo and Paynabar, Kamran and Shi, Jianjun and Domain, Jianjun Shi Frequency},
  year = {2014},
  month = jul,
  pages = {1022998},
  address = {{Nantes, France.}},
  abstract = {Guided wavefield detection is at the basis of a number of promising techniques for the identification and the characterization of damage in plate structures. Among the processing techniques proposed, the estimation of instantaneous wavenumbers can be used as an effective metric that localize and quantifies delaminations in composite plates. A process able to estimate the in-plane and out-of-plane (depth) coordinate of a feature in a 2D structure using the Frequency Domain Instantaneous Wavenumber (FDIW) damage quan-tification technique is detailed in this paper. A post processing algorithm using a smooth sparse decomposition is used to highlight the studied features. The effectiveness of this method combined to the post processing technique is demonstrated for both numerical and experimental cases. This proposed methodology can be considered as a first step towards a hybrid structural health monitoring/ nondestructive evaluation (SHM/NDE) approach for damage assessment in composites.},
  file = {/Users/hyan46/Dropbox (ASU)/ZoteroData/Mesnil et al_2014_Frequency Domain Instantaneous Wavenumber Estimation for Damage Quantification2.pdf},
  keywords = {Anomaly Detection,Material}
}

@article{reisigahrooeiComments2020,
  title = {Comments on: {{On Active Learning Methods}} for {{Manifold Data}}},
  shorttitle = {Comments On},
  author = {Reisi Gahrooei, Mostafa and Yan, Hao and Paynabar, Kamran},
  year = {2020},
  month = mar,
  volume = {29},
  pages = {38--41},
  issn = {1133-0686, 1863-8260},
  doi = {10.1007/s11749-019-00696-w},
  copyright = {All rights reserved},
  file = {/Users/hyan46/Dropbox (ASU)/ZoteroData/Reisi Gahrooei et al_2020_Comments on2.pdf},
  journal = {TEST},
  language = {en},
  number = {1}
}

@article{sergin2021toward,
  title = {Toward a Better Monitoring Statistic for Profile Monitoring via Variational Autoencoders},
  author = {Sergin, Nurettin Dorukhan and Yan, Hao},
  year = {2021},
  pages = {1--46},
  publisher = {{Taylor \& Francis}},
  doi = {10.1080/00224065.2021.1903821},
  abstract = {Variational autoencoders have been recently proposed for the problem of process monitoring. While these works show impressive results over classical methods, the proposed monitoring statistics often ignore the inconsistencies in learned lower-dimensional representations and computational limitations in high-dimensional approximations. In this work, we first manifest these issues and then overcome them with a novel statistic formulation that increases out-of-control detection accuracy without compromising computational efficiency. We demonstrate our results on a simulation study with explicit control over latent variations, and a real-life example of image profiles obtained from a hot steel rolling process.},
  copyright = {All rights reserved},
  file = {/Users/hyan46/Dropbox (ASU)/ZoteroData/Sergin_Yan_2021_Toward a better monitoring statistic for profile monitoring via variational.pdf},
  journal = {Journal of Quality Technology},
  keywords = {Anomaly Detection,Deep Learning,Image,Manufacturing}
}

@article{wuAdaptive2021,
  title = {Adaptive {{Change Point Monitoring}} for {{High}}-{{Dimensional Data}}},
  author = {Wu, Teng and Wang, Runmin and Yan, Hao and Shao, Xiaofeng},
  year = {2021},
  doi = {10.5705/ss.202020.0438},
  abstract = {In this paper, we propose a class of monitoring statistics for a mean shift in a sequence of high-dimensional observations. Inspired by the recent U-statistic based retrospective tests developed by Wang et al. (2019) and Zhang et al. (2020), we advance the U-statistic based approach to the sequential monitoring problem by developing a new adaptive monitoring procedure that can detect both dense and sparse changes in real time. Unlike Wang et al. (2019) and Zhang et al. (2020), where self-normalization was used in their tests, we instead introduce a class of estimators for q-norm of the covariance matrix and prove their ratio consistency. To facilitate fast computation, we further develop recursive algorithms to improve the computational efficiency of the monitoring procedure. The advantage of the proposed methodology is demonstrated via simulation studies and real data illustrations.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  eprint = {2101.06839},
  eprinttype = {arxiv},
  file = {/Users/hyan46/Dropbox (ASU)/ZoteroData/Wu et al_2021_Adaptive Change Point Monitoring for High-Dimensional Data.pdf},
  journal = {Statistica Sinica},
  keywords = {Anomaly Detection,Signal},
  language = {en}
}

@article{yanAnomaly2017,
  title = {Anomaly {{Detection}} in {{Images With Smooth Background}} via {{Smooth}}-{{Sparse Decomposition}}},
  author = {Yan, Hao and Paynabar, Kamran and Shi, Jianjun},
  year = {2017},
  month = jan,
  volume = {59},
  pages = {102--114},
  publisher = {{American Statistical Association}},
  issn = {15372723},
  doi = {10.1080/00401706.2015.1102764},
  abstract = {In various manufacturing applications such as steel, composites, and textile production, anomaly detection in noisy images is of special importance. Although there are several methods for image denoising and anomaly detection, most of these perform denoising and detection sequentially, which affects detection accuracy and efficiency. Additionally, the low computational speed of some of these methods is a limitation for real-time inspection. In this article, we develop a novel methodology for anomaly detection in noisy images with smooth backgrounds. The proposed method, named smooth-sparse decomposition, exploits regularized high-dimensional regression to decompose an image and separate anomalous regions by solving a large-scale optimization problem. To enable the proposed method for real-time implementation, a fast algorithm for solving the optimization model is proposed. Using simulations and a case study, we evaluate the performance of the proposed method and compare it with existing methods. Numerical results demonstrate the superiority of the proposed method in terms of the detection accuracy as well as computation time. This article has supplementary materials that includes all the technical details, proofs, MATLAB codes, and simulated images used in the article.},
  file = {/Users/hyan46/Dropbox (ASU)/ZoteroData/Yan et al_2017_Anomaly Detection in Images With Smooth Background via Smooth-Sparse2.pdf},
  journal = {Technometrics},
  keywords = {Anomaly Detection,Functional Data,Image,Manufacturing,Material,Sparse Learning},
  number = {1}
}

@techreport{yanHIGH2017,
  title = {{{HIGH DIMENSIONAL DATA ANALYSIS FOR ANOMALY DETECTION AND QUALITY IMPROVEMENT A Dissertation Presented}} to {{The Academic Faculty}}},
  author = {Yan, Hao},
  year = {2017},
  month = jun,
  institution = {{Georgia Institute of Technology}},
  abstract = {Analysis of large-scale high-dimensional data with a complex heterogeneous data structure to extract information or useful features is vital for the purpose of data fusion for assessment of system performance, early detection of system anomalies, intelligent sampling and sensing for data collection and decision making to achieve optimal system performance. Chapter 3 focuses on detecting anomalies from high-dimensional data. Traditionally, most of the image-based anomaly detection methods perform denoising and detection sequentially, which affects detection accuracy and efficiency. In this chapter, A novel methodology, named smooth-sparse decomposition (SSD), is proposed to exploit regularized high-dimensional regression to decompose an image and separate anomalous regions simultaneously by solving a large-scale optimization problem. Chapter 4 extends this to spatial-temporal functional data by extending SSD to spatiotemporal smooth-sparse decomposition (ST-SSD), with a likelihood ratio test to detect the time of change accurately based on the detected anomaly. To enable real-time implementation of the proposed methodology, recursive estimation procedures for ST-SSD are also developed. The proposed methodology is also applied to tonnage signals, rolling inspection data and solar flare monitoring. Chapter 5 considers the adaptive sampling problem for high-dimensional data. A novel adaptive sampling framework, named Adaptive Kernelized Maximum-Minimum Distance is proposed to adaptively estimate the sparse anomalous region. The proposed method balances the sampling efforts between the space filling sampling (exploration) and focused sampling near the anomalous region (exploitation). The proposed methodology is also applied to a case study of anomaly detection in composite sheets using a guided wave test. Chapter 6 explores the penalized tensor regression to model the tensor response data with the process variables. Regularized Tucker decomposition and regularized tensor regression methods are developed, which model the structured point cloud data as tensors and link the point cloud data with the process variables. The performance of the proposed method is evaluated through simulation and a real case study of turning process optimization.},
  file = {/Users/hyan46/Dropbox (ASU)/ZoteroData/Yan_2017_HIGH DIMENSIONAL DATA ANALYSIS FOR ANOMALY DETECTION AND QUALITY IMPROVEMENT A2.pdf},
  keywords = {Anomaly Detection,Functional Data,Tensor}
}

@article{yanImagebased2015,
  title = {Image-Based Process Monitoring Using Low-Rank Tensor Decomposition},
  author = {Yan, Hao and Paynabar, Kamran and Shi, Jianjun},
  year = {2015},
  month = jan,
  volume = {12},
  pages = {216--227},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {15455955},
  doi = {10.1109/TASE.2014.2327029},
  abstract = {Image and video sensors are increasingly being deployed in complex systems due to the rich process information that these sensors can capture. As a result, image data play an important role in process monitoring and control in different application domains such as manufacturing processes, food industries, medical decision-making, and structural health monitoring. Existing process monitoring techniques fail to fully utilize the information of color images due to their complex data characteristics including the high-dimensionality and correlation structure (i.e., temporal, spatial and spectral correlation). This paper proposes a new image-based process monitoring approach that is capable of handling both grayscale and color images. The proposed approach models the high-dimensional structure of the image data with tensors and employs low-rank tensor decomposition techniques to extract important monitoring features monitored using multivariate control charts. In addition, this paper shows the analytical relationships between different low-rank tensor decomposition methods. The performance of the proposed method in quick detection of process changes is evaluated and compared with existing methods through extensive simulations and a case study in a steel tube manufacturing process. Note to Practitioners - This paper, motivated by the problem of combustion monitoring in steel tube manufacturing, focuses on the development of effective methods for process monitoring based on image data. Existing process monitoring techniques cannot fully utilize the information of color images due to the high-dimensionality and complex correlation structure of such data. This paper addresses this problem by extracting essential monitoring features, while considering the spatial and spectral correlation of color images. This is accomplished by using various low-rank tensor decomposition methods along with multivariate control charts. The proposed approach can lead to a computer-aided online monitoring system for automatic detection of out-of-control situations in a process. Using simulation, the performance of the developed methods is compared under various scenarios. This can provide practitioners with useful guidelines for selecting an appropriate method for image-based process monitoring. In future research, we will study the development of image-based fault diagnosis techniques that can be integrated with the process monitoring approaches proposed in this paper.},
  file = {/Users/hyan46/Dropbox (ASU)/ZoteroData/Yan et al_2015_Image-based process monitoring using low-rank tensor decomposition2.pdf},
  journal = {IEEE Transactions on Automation Science and Engineering},
  keywords = {Anomaly Detection,Image,Manufacturing,Tensor},
  number = {1}
}

@inproceedings{yanImagebased2019,
  title = {Image-Based Process Monitoring via Adversarial Autoencoder with Applications to Rolling Defect Detection},
  booktitle = {{{IEEE International Conference}} on {{Automation Science}} and {{Engineering}}},
  author = {Yan, Hao and Yeh, Huai Ming and Sergin, Nurettin},
  year = {2019},
  month = aug,
  volume = {2019-Augus},
  pages = {311--316},
  publisher = {{IEEE Computer Society}},
  issn = {21618089},
  doi = {10.1109/COASE.2019.8843313},
  abstract = {Image-based process monitoring has recently attracted increasing attention due to the advancement of the sensing technologies. However, existing process monitoring methods fail to fully utilize the spatial information of images due to their complex characteristics including the high-dimensionality and complex spatial structures. Recent advancements in unsupervised deep models such as generative adversarial networks (GAN) and adversarial autoencoders (AAE) has enabled to learn the complex spatial structures automatically. Inspired by this advancement, we propose an anomaly detection framework based on the AAE for unsupervised anomaly detection for images. AAE combines the power of GAN with the variational autoencoder, which serves as a nonlinear dimension reduction technique. Based on this, we propose a monitoring statistic efficiently capturing the change of the data. The performance of the proposed AAE-based anomaly detection algorithm is validated through a simulation study and real case study for rolling defect detection.},
  file = {/Users/hyan46/Dropbox (ASU)/ZoteroData/Yan et al_2019_Image-based process monitoring via adversarial autoencoder with applications to2.pdf},
  isbn = {978-1-72810-355-6},
  keywords = {Anomaly Detection,Deep Learning,Image,Manufacturing}
}

@article{yanMultiple2016,
  title = {Multiple {{Sensor Data Fusion}} for {{Degradation Modeling}} and {{Prognostics}} under {{Multiple Operational Conditions}}},
  author = {Yan, Hao and Liu, Kaibo and Zhang, Xi and Shi, Jianjun},
  year = {2016},
  month = sep,
  volume = {65},
  pages = {1416--1426},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {00189529},
  doi = {10.1109/TR.2016.2575449},
  abstract = {Due to the rapid advances in sensing and computing technology, multiple sensors have been widely used to simultaneously monitor the health status of an operation unit. This creates a data-rich environment, enabling an unprecedented opportunity to make better understanding and inference about the current and future behavior of the unit in real time. Depending on specific task requirements, a unit is often required to run under multiple operational conditions, each of which may affect the degradation path of the unit differently. Thus, two fundamental challenges remain to be solved for effective degradation modeling and prognostic analysis: 1) how to leverage the dependent information among multiple sensor signals to better understand the health condition of the unit; and 2) how to model the effects of multiple conditions on the degradation characteristics of the unit. To address these two issues, this paper develops a data fusion methodology that integrates the information from multiple sensors to construct a health index when the monitored unit runs under multiple operational conditions. Our goal is that the developed health index provides a much better characterization of the health condition of the degraded unit, and, thus, leads to a better prediction of the remaining lifetime. Unlike other existing approaches, the developed data fusion model combines the fusion procedure and the degradation modeling under different operational conditions in a unified manner. The effectiveness of the proposed method is demonstrated in a case study, which involves a degradation dataset of aircraft gas turbine engines collected from 21 sensors under six different operational conditions.},
  copyright = {All rights reserved},
  file = {/Users/hyan46/Dropbox (ASU)/ZoteroData/Yan et al_2016_Multiple Sensor Data Fusion for Degradation Modeling and Prognostics under3.pdf},
  journal = {IEEE Transactions on Reliability},
  keywords = {Data fusion,multiple sensors,Prognostics},
  number = {3}
}

@inproceedings{yanPhysicsbased2019,
  title = {Physics-Based Deep Spatio-Temporal Metamodeling for Cardiac Electrical Conduction Simulation},
  booktitle = {{{IEEE International Conference}} on {{Automation Science}} and {{Engineering}}},
  author = {Yan, Hao and Zhao, Xinyu and Hu, Zhiyong and Du, Dongping},
  year = {2019},
  month = aug,
  volume = {2019-Augus},
  pages = {152--157},
  publisher = {{IEEE Computer Society}},
  issn = {21618089},
  doi = {10.1109/COASE.2019.8842902},
  abstract = {Modeling and simulation have been widely used in both cardiac research and clinical study to investigate cardiac disease mechanism and develop new treatment design. Electrical conduction among cardiac tissue is commonly modeled with a partial differential equation, i.e., reaction-diffusion equation, where the reaction term describes cellular excitation and diffusion term describes electrical propagation. Cellular excitation can be modeled by either detailed human cellular models or simplified models such as the FitzHugh-Nagumo model; electrical propagation can be simulated using either biodomain or mono-domain tissue model. However, existing cardiac models have a great level of complexity, and the simulation is often time-consuming. This paper develops a new spatiotemporal model as a surrogate model of the timeconsuming cardiac model. Specifically, we propose to investigate the auto-regressive convolutional neural network (AR-CNN) and convolutional long short-term memory (Conv-LSTM) to model the spatial and temporal structure for the metamodeling. Model predictions are compared to the one-dimensional simulation data to validate the prediction accuracy. The metamodel can accurately capture the properties of the individual cardiac cell, as well as the electrical wave morphology in cardiac fiber at different simulation scenarios, which demonstrates its superior performance in modeling and the long-term prediction.},
  file = {/Users/hyan46/Dropbox (ASU)/ZoteroData/Yan et al_2019_Physics-based deep spatio-temporal metamodeling for cardiac electrical2.pdf},
  isbn = {978-1-72810-355-6},
  keywords = {Cardiac,Deep Learning,Physics}
}

@article{yanRealTime2018,
  title = {Real-{{Time Monitoring}} of {{High}}-{{Dimensional Functional Data Streams}} via {{Spatio}}-{{Temporal Smooth Sparse Decomposition}}},
  author = {Yan, Hao and Paynabar, Kamran and Shi, Jianjun},
  year = {2018},
  month = apr,
  volume = {60},
  pages = {181--197},
  publisher = {{American Statistical Association}},
  issn = {15372723},
  doi = {10.1080/00401706.2017.1346522},
  abstract = {High-dimensional data monitoring and diagnosis has recently attracted increasing attention among researchers as well as practitioners. However, existing process monitoring methods fail to fully use the information of high-dimensional data streams due to their complex characteristics including the large dimensionality, spatio-temporal correlation structure, and nonstationarity. In this article, we propose a novel process monitoring methodology for high-dimensional data streams including profiles and images that can effectively address foregoing challenges. We introduce spatio-temporal smooth sparse decomposition (ST-SSD), which serves as a dimension reduction and denoising technique by decomposing the original tensor into the functional mean, sparse anomalies, and random noises. ST-SSD is followed by a sequential likelihood ratio test on extracted anomalies for process monitoring. To enable real-time implementation of the proposed methodology, recursive estimation procedures for ST-SSD are developed. ST-SSD also provides useful diagnostics information about the location of change in the functional mean. The proposed methodology is validated through various simulations and real case studies. Supplementary materials for this article are available online.},
  copyright = {All rights reserved},
  file = {/Users/hyan46/Dropbox (ASU)/ZoteroData/Yan et al_2018_Real-Time Monitoring of High-Dimensional Functional Data Streams via3.pdf},
  journal = {Technometrics},
  keywords = {Anomaly Detection,Functional Data,Image,Material,Spatio-temporal,Tensor},
  number = {2}
}

@article{yanRealtime2021,
  title = {Real-Time {{Detection}} of {{Clustered Events}} in {{Video}}-Imaging Data with {{Applications}} to {{Additive Manufacturing}}},
  author = {Yan, Hao and Grasso, Marco and Paynabar, Kamran and Colosimo, Bianca Maria},
  year = {2021},
  doi = {10.1080/24725854.2021.1882013},
  abstract = {The use of video-imaging data for in-line process monitoring applications has become more and more popular in the industry. In this framework, spatio-temporal statistical process monitoring methods are needed to capture the relevant information content and signal possible out-of-control states. Video-imaging data are characterized by a spatio-temporal variability structure that depends on the underlying phenomenon, and typical out-of-control patterns are related to the events that are localized both in time and space. In this paper, we propose an integrated spatio-temporal decomposition and regression approach for anomaly detection in video-imaging data. Out-of-control events are typically sparse spatially clustered and temporally consistent. Therefore, the goal is to not only detect the anomaly as quickly as possible ("when") but also locate it ("where"). The proposed approach works by decomposing the original spatio-temporal data into random natural events, sparse spatially clustered and temporally consistent anomalous events, and random noise. Recursive estimation procedures for spatio-temporal regression are presented to enable the real-time implementation of the proposed methodology. Finally, a likelihood ratio test procedure is proposed to detect when and where the hotspot happens. The proposed approach was applied to the analysis of video-imaging data to detect and locate local over-heating phenomena ("hotspots") during the layer-wise process in a metal additive manufacturing process.},
  annotation = {\_eprint: 2004.10977},
  file = {/Users/hyan46/Dropbox (ASU)/ZoteroData/Yan et al_2021_Real-time Detection of Clustered Events in Video-imaging data with Applications.pdf},
  journal = {IISE Transactions},
  keywords = {Additive Manufacturing,Anomaly Detection,Manufacturing,Video}
}

@article{yanStructured2019,
  title = {Structured {{Point Cloud Data Analysis Via Regularized Tensor Regression}} for {{Process Modeling}} and {{Optimization}}},
  author = {Yan, Hao and Paynabar, Kamran and Pacella, Massimo},
  year = {2019},
  month = jul,
  volume = {61},
  pages = {385--395},
  publisher = {{American Statistical Association}},
  issn = {15372723},
  doi = {10.1080/00401706.2018.1529628},
  abstract = {Advanced 3D metrology technologies such as coordinate measuring machine and laser 3D scanners have facilitated the collection of massive point cloud data, beneficial for process monitoring, control and optimization. However, due to their high dimensionality and structure complexity, modeling and analysis of point clouds are still a challenge. In this article, we use multilinear algebra techniques and propose a set of tensor regression approaches to model the variational patterns of point clouds and to link them to process variables. The performance of the proposed methods is evaluated through simulations and a real case study of turning process optimization.},
  annotation = {\_eprint: 1807.10278},
  copyright = {All rights reserved},
  file = {/Users/hyan46/Dropbox (ASU)/ZoteroData/Yan et al_2019_Structured Point Cloud Data Analysis Via Regularized Tensor Regression for3.pdf},
  journal = {Technometrics},
  keywords = {3D Point Cloud,Anomaly Detection,Manufacturing,Tensor},
  number = {3}
}

@article{yueGeneralized2017,
  title = {Generalized {{Wavelet Shrinkage}} of {{Inline Raman Spectroscopy}} for {{Quality Monitoring}} of {{Continuous Manufacturing}} of {{Carbon Nanotube Buckypaper}}},
  author = {Yue, Xiaowei and Wang, Kan and Yan, Hao and Park, Jin Gyu and Liang, Zhiyong and Zhang, Chuck and Wang, Ben and Shi, Jianjun},
  year = {2017},
  month = jan,
  volume = {14},
  pages = {196--207},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {15455955},
  doi = {10.1109/TASE.2016.2599023},
  abstract = {Process monitoring and quality control is essential for continuous manufacturing processes of carbon nano- tube (CNT) thin sheets or buckypaper. Raman spectroscopy is an attractive inline quality characterization and quantification tool for nanomanufacturing because of its nondestructive nature, fast data acquisition speed, and ability to provide detailed material information. However, there is signal-dependent noise buried in the Raman spectra, which reduces the signal-to-noise (S/N) ratio and affects the accuracy, efficiency, and sensitivity for Raman spectrum-based quality control approaches. In this paper, a signal analysis model with signal-dependent noise for Raman spectroscopy is developed and validated based on experimental data. The wavelet shrinkage method is used for denoising and improving the S/N ratio of raw Raman spectra. Based on the validated signal-noise relationship, a novel generalized wavelet shrinkage approach is introduced to remove noise in all wavelet coefficients by applying individual adaptive wavelet thresholds. The effectiveness of this method is demonstrated using both simulation and experimental case studies of inline Raman monitoring of continuous buckypaper manufacturing. The proposed method allows for a significant reduction of Raman data acquisition time without much loss of S/N ratio, which inherently enables Raman spectroscopy for inline monitoring and control for continuous nanomanufacturing processes.},
  file = {/Users/hyan46/Dropbox (ASU)/ZoteroData/Yue et al_2017_Generalized Wavelet Shrinkage of Inline Raman Spectroscopy for Quality2.pdf},
  journal = {IEEE Transactions on Automation Science and Engineering},
  keywords = {Anomaly Detection,Functional Data,Manufacturing,Material,Sparse Learning},
  number = {1}
}

@article{yueWaveletBased2018,
  title = {A {{Wavelet}}-{{Based Penalized Mixed}}-{{Effects Decomposition}} for {{Multichannel Profile Detection}} of {{In}}-{{Line Raman Spectroscopy}}},
  author = {Yue, Xiaowei and Yan, Hao and Park, Jin Gyu and Liang, Zhiyong and Shi, Jianjun},
  year = {2018},
  month = jul,
  volume = {15},
  pages = {1258--1271},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {15455955},
  doi = {10.1109/TASE.2017.2772218},
  abstract = {Modeling and analysis of profiles, especially high-dimensional nonlinear profiles, is an important and challenging topic in statistical process control. Conventional mixed-effects models have several limitations in solving the multichannel profile detection problems for in-line Raman spectroscopy, such as the inability to separate defective information from random effects, computational inefficiency, and inability to handle high-dimensional extracted coefficients. In this paper, a new wavelet-based penalized mixed-effects decomposition (PMD) method is proposed to solve the multichannel profile detection problem in Raman spectroscopy. The proposed PMD exploits a regularized high-dimensional regression with linear constraints to decompose the profiles into four parts: fixed effects, normal effects, defective effects, and signal-dependent noise. An optimization algorithm based on the accelerated proximal gradient (APG) is developed to do parameter estimation efficiently for the proposed model. Finally, the separated fixed effects coefficients, normal effects coefficients, and defective effects coefficients can be used to extract the quality features of fabrication consistency, within-sample uniformity, and defect information, respectively. Using a surrogated data analysis and a case study, we evaluated the performance of the proposed PMD method and demonstrated a better detection power with less computational time. Note to Practitioners - This paper was motivated by the need of implementing multichannel profile detection for Raman spectra to realize in-line process monitoring and quality control of continuous manufacturing of carbon nanotube (CNT) buckypaper. Existing approaches, such as the mixed-effects model or the smooth-sparse decomposition method, cannot separate defective information in random effects effectively. This paper develops a penalized mixed-effects decomposition which decomposes Raman spectra into four components: fixed effects, normal effects, defective effects, and signal-dependent noise, respectively. The first three components can be applied to monitor the fabrication consistency, degree of uniformity, and defect information of buckypaper, respectively. With this new approach, several quality features can be monitored simultaneously and the algorithm based on the accelerated proximal gradient (APG) method can satisfy the computation speed requirement of in-line monitoring. This paper provides a solid foundation for in-line process monitoring and quality control for scalable nanomanufacturing of CNT buckypaper. Furthermore, the developed methodology can be applied in the decomposition of other signal systems with fixed, normal, and defective effects.},
  file = {/Users/hyan46/Dropbox (ASU)/ZoteroData/Yue et al_2018_A Wavelet-Based Penalized Mixed-Effects Decomposition for Multichannel Profile2.pdf},
  journal = {IEEE Transactions on Automation Science and Engineering},
  keywords = {Anomaly Detection,Functional Data,Manufacturing,Material,Profiles},
  number = {3}
}

@article{zhangDynamic2020,
  title = {Dynamic {{Multivariate Functional Data Modeling}} via {{Sparse Subspace Learning}}},
  author = {Zhang, Chen and Yan, Hao and Lee, Seungho and Shi, Jianjun},
  year = {2020},
  doi = {10.1080/00401706.2020.1800516},
  abstract = {Multivariate functional data from a complex system are naturally high-dimensional and have complex cross-correlation structure. The complexity of data structure can be observed as that (1) some functions are strongly correlated with similar features, while some others may have almost no cross-correlations with quite diverse features; and (2) the cross-correlation structure may also change over time due to the system evolution. With this regard, this paper presents a dynamic subspace learning method for multivariate functional data modeling. In particular, we consider different functions come from different subspaces, and only functions of the same subspace have cross-correlations with each other. The subspaces can be automatically formulated and learned by reformatting the problem as a sparse regression. By allowing but regularizing the regression change over time, we can describe the cross-correlation dynamics. The model can be efficiently estimated by the fast iterative shrinkage-thresholding algorithm (FISTA), and the features of every subspace can be extracted using the smooth multi-channel functional PCA. Numerical studies together with case studies demonstrate the efficiency and applicability of the proposed methodology.},
  annotation = {\_eprint: 1804.03797},
  copyright = {All rights reserved},
  file = {/Users/hyan46/Dropbox (ASU)/ZoteroData/Zhang et al_2020_Dynamic Multivariate Functional Data Modeling via Sparse Subspace Learning2.pdf},
  journal = {Technometrics},
  keywords = {Functional Data,Human Motion,Manufacturing,Profiles,Sparse Learning}
}

@article{zhangMultiple2018,
  title = {Multiple Profiles Sensor-Based Monitoring and Anomaly Detection},
  author = {Zhang, Chen and Yan, Hao and Lee, Seungho and Shi, Jianjun},
  year = {2018},
  volume = {50},
  pages = {344--362},
  publisher = {{American Society for Quality}},
  issn = {00224065},
  doi = {10.1080/00224065.2018.1508275},
  abstract = {Generally, in an advanced manufacturing system hundreds of sensors are deployed to measure key process variables in real time. Thus it is desirable to develop methodologies to use real-time sensor data for on-line system condition monitoring and anomaly detection. However, there are several challenges in developing an effective process monitoring system: (i) data streams generated by multiple sensors are high-dimensional profiles; (ii) sensor signals are affected by noise due to system-inherent variations; (iii) signals of different sensors have cluster-wise features; and (iv) an anomaly may cause only sparse changes of sensor signals. To address these challenges, this article presents a real-time multiple profiles sensor-based process monitoring system, which includes the following modules: (i) preprocessing sensor signals to remove inherent variations and conduct profile alignments, (ii) using multichannel functional principal component analysis (MFPCA)\textendash based methods to extract sensor features by considering cluster-wise between-sensor correlations, and (iii) constructing a monitoring scheme with the top-R strategy based on the extracted features, which has scalable detection power for different fault patterns. Finally, we implement and demonstrate the proposed framework using data from a real manufacturing system.},
  copyright = {All rights reserved},
  file = {/Users/hyan46/Dropbox (ASU)/ZoteroData/Zhang et al_2018_Multiple profiles sensor-based monitoring and anomaly detection2.pdf},
  journal = {Journal of Quality Technology},
  keywords = {Anomaly Detection,Manufacturing,Profiles},
  number = {4}
}

@article{zhangWeakly2018,
  title = {Weakly Correlated Profile Monitoring Based on Sparse Multi-Channel Functional Principal Component Analysis},
  author = {Zhang, Chen and Yan, Hao and Lee, Seungho and Shi, Jianjun},
  year = {2018},
  month = oct,
  volume = {50},
  pages = {878--891},
  publisher = {{Taylor and Francis Ltd.}},
  issn = {24725862},
  doi = {10.1080/24725854.2018.1451012},
  abstract = {Although several works have been proposed for multi-channel profile monitoring, two additional challenges are yet to be addressed: (i) how to model complex correlations of multi-channel profiles when different profiles have different features (i.e., weakly or sparsely correlated); (ii) how to efficiently detect sparse changes occurring in only a small segment of a few profiles. To fill this research gap, our contributions are twofold. First, we propose a novel Sparse Multi-channel Functional Principal Component Analysis (SMFPCA) to model multi-channel profiles. SMFPCA can not only flexibly describe the correlation structure of multiple, or even high-dimensional, profiles with distinct features, but also achieve sparse PCA scores which are easily interpretable. Second, we propose an efficient convergence-guaranteed optimization algorithm to solve SMFPCA in real time based on the block coordinate descent algorithm. Third, as the SMFPCA scores can naturally identify sparse out-of-control (OC) patterns, we use the scores to construct a monitoring scheme which provides increased sensitivity to sparse OC changes. Numerical studies together with a real case study in a manufacturing system demonstrate the effectiveness of the developed methodology.},
  copyright = {All rights reserved},
  file = {/Users/hyan46/Dropbox (ASU)/ZoteroData/Zhang et al_2018_Weakly correlated profile monitoring based on sparse multi-channel functional2.pdf},
  journal = {IISE Transactions},
  keywords = {Anomaly Detection,Functional Data,Manufacturing,Profiles,Sparse Learning},
  number = {10}
}

@inproceedings{zhaoHierarchical2021,
  title = {Hierarchical {{Tree}}-Based {{Sequential Event Prediction}} with {{Application}} in the {{Aviation Accident Report}}},
  booktitle = {Proceedings 37th {{IEEE}} International Conference on Data Engineering},
  author = {Zhao, Xinyu and Yan, Hao and Liu, Yongming},
  year = {2021},
  doi = {10.1109/ICDE51399.2021.00178},
  abstract = {Sequential event prediction is a well-studied area and has been widely used in proactive management, recommender systems and healthcare. One major assumption of the existing sequential event prediction methods is that similar event sequence patterns in the historical record will repeat themselves, enabling us to predict future events. However, in reality, the assumption becomes less convincing when we are trying to predict rare or unique sequences. Furthermore, the representation of the event may be complex with hierarchical structures. In this paper, we aim to solve this issue by taking advantage of the multi-level or hierarchical representation of these rare events. We proposed to build a sequential Encoder-Decoder framework to predict the event sequences. More specifically, in the encoding layer, we built a hierarchical embedding representation for the events. In the decoding layer, we first predict the high-level events and the low-level events are generated according to a hierarchical graphical structure. We propose to link the encoding decoding layers with the temporal models for future event prediction. In this article, we further discussed applying the proposed model into the failure event prediction according to the aviation accident reports and have shown improved accuracy and model interpretability.},
  copyright = {All rights reserved},
  keywords = {Classification,Deep Learning,Text,Traffic}
}

@inproceedings{zhaoRapid2019,
  title = {Rapid {{Detection}} of {{Hot}}-Spot by {{Tensor Decomposition}} with {{Application}} to {{Weekly Gonorrhea Data}}},
  booktitle = {The {{XIIIth International Workshop}} on {{Intelligent Statistical Quality Control}},},
  author = {Zhao, Yujie and Yan, Hao and Holte, Sarah E. and Kerani, Roxanne P. and Mei, Yajun},
  year = {2019},
  month = jan,
  pages = {289--310},
  address = {{Hong Kong}},
  doi = {10.1080/00224065.2021.1903822},
  abstract = {In many bio-surveillance and healthcare applications, data sources are measured from many spatial locations repeatedly over time, say, daily/weekly/monthly. In these applications, we are typically interested in detecting hot-spots, which are defined as some structured outliers that are sparse over the spatial domain but persistent over time. In this paper, we propose a tensor decomposition method to detect when and where the hot-spots occur. Our proposed methods represent the observed raw data as a three-dimensional tensor including a circular time dimension for daily/weekly/monthly patterns, and then decompose the tensor into three components: smooth global trend, local hot-spots, and residuals. A combination of LASSO and fused LASSO is used to estimate the model parameters, and a CUSUM procedure is applied to detect when and where the hot-spots might occur. The usefulness of our proposed methodology is validated through numerical simulation and a real-world dataset in the weekly number of gonorrhea cases from \$2006\$ to \$2018\$ for \$50\$ states in the United States.},
  annotation = {\_eprint: 2001.11685},
  file = {/Users/hyan46/Dropbox (ASU)/ZoteroData/Zhao et al_2019_Rapid Detection of Hot-spot by Tensor Decomposition with Application to Weekly2.pdf},
  keywords = {Anomaly Detection,Health,Tensor,Time Series}
}

@article{zhaoRapid2021,
  title = {Rapid {{Detection}} of {{Hot}}-Spots via {{Tensor Decomposition}} with Applications to {{Crime Rate Data}}},
  author = {Zhao, Yujie and Yan, Hao and Holte, Sarah and Mei, Yajun},
  year = {2021},
  doi = {10.1080/02664763.2021.1874892},
  abstract = {We propose an efficient statistical method (denoted as SSR-Tensor) to robustly and quickly detect hot-spots that are sparse and temporal-consistent in a spatial-temporal dataset through the tensor decomposition. Our main idea is to first build an SSR model to decompose the tensor data into a Smooth global trend mean, Sparse local hot-spots and Residuals. Next, tensor decomposition is utilized as follows: basis are introduced to describe within-dimension correlation and tensor products are used for between-dimension interaction. Then, a combination of LASSO and fused LASSO is used to estimate the model parameters, where an efficient recursive estimation procedure is developed based on the large-scale convex optimization, where we first transform the general LASSO optimization into regular LASSO optimization and apply FISTA to solve it with the fastest convergence rate. Finally, a CUSUM procedure is applied to detect when and where the hot-spot event occurs. We compare the performance of the proposed method in a numerical simulation and a real-world dataset, which is a collection of three types of crime rates for U.S. mainland states during the year 1965-2014. In both cases, the proposed SSR-Tensor is able to achieve the fast detection and accurate localization of the hot-spots.},
  annotation = {\_eprint: 2004.11710},
  file = {/Users/hyan46/Dropbox (ASU)/ZoteroData/Zhao et al_2021_Rapid Detection of Hot-spots via Tensor Decomposition with applications to.pdf},
  journal = {Journal of Applied Statistics},
  keywords = {Anomaly Detection,Social,Tensor,Video}
}

@inproceedings{zhaoSimultaneous2021,
  title = {Simultaneous {{Material Microstructure Classification}} and {{Discovery}} via {{Hidden Markov Modeling}} of {{Acoustic Emission Signals}}},
  booktitle = {{{ASME}} 2020 15th {{International Manufacturing Science}} and {{Engineering Conference}}},
  author = {Zhao, Xinyu and Iquebal, Ashif and Sun, Huifeng and Yan, Hao},
  year = {2021},
  month = jan,
  publisher = {{American Society of Mechanical Engineers Digital Collection}},
  doi = {10.1115/MSEC2020-8454},
  abstract = {Acoustic emission (AE) signals have been widely employed for tracking material properties and structural characteristics. In this study, we aim to analyze the AE signals gathered during a scanning probe lithography process to classify the known microstructure types and discover unknown surface microstructures/anomalies. To achieve this, we developed a Hidden Markov Model to consider the temporal dependency of the high-resolution AE data. Furthermore, we compute the posterior classification probability and the negative likelihood score for microstructure classification and discovery. Subsequently, we present a diagnostic procedure to identify the dominant AE frequencies that allow us to track the microstructural characteristics. Finally, we apply the proposed approach to identify the surface microstructures of additively manufactured Ti-6Al-4V and show that it not only achieved a high classification accuracy (e.g., more than 90\%) but also correctly identified the microstructural anomalies that may be subjected further investigation to discover new material phases/properties.},
  copyright = {All rights reserved},
  keywords = {Additive Manufacturing,Classification,Temporal Learning,Time Series},
  language = {en}
}

@article{zhaoSpatiotemporal2019,
  title = {Spatio-Temporal {{Anomaly Detection}}, {{Diagnostics}}, and {{Prediction}} of the {{Air}}-Traffic {{Trajectory Deviation}} Using the {{Convective Weather}}},
  author = {Zhao, Xinyu and Yan, Hao and Li, Jing and Pang, Yutian and Liu, Yongming},
  year = {2019},
  month = sep,
  volume = {11},
  issn = {2325-0178},
  doi = {10.36001/phmconf.2019.v11i1.854},
  abstract = {With ahead-of-time aircraft management, we are able to re- duce aircraft collision and improve air traffic capacity. How- ever, there are various impact factors which will cause a large deviation between the actual flight and the original flight plan. Such uncertainty will result in an inappropriate decision for flight management. In order to solve this problem, most of the existing research attempt to build up a stochastic trajec- tory prediction model to capture the influence of the weather. However, the complexity of the weather information and vari- ous human factors make it hard to build up an accurate trajec- tory prediction framework. Our approach considers the prob- lem of trajectory deviation as the ''anomaly'' and builds up an analytics pipeline for anomaly detection, anomaly diagnos- tics, and anomaly prediction. For anomaly detection, we pro- pose to apply the CUSUM chart to detect the abnormal tra- jectory point which differs from the flight plan. For anomaly diagnostics, we would like to link the entire anomalous trajec- tory sequences with the convective weather data and extract important features based on time-series feature engineering. Furthermore, XGBoost was applied to detect the anomalous trajectory sequences based on the time-series features. For anomaly prediction, we will build up a point-wise prediction framework based on the Hidden Markov Model and Convectional LSTM to predict the probability that the pilot would deviate from the flight plan. Finally, we demonstrate the sig- nificance of the proposed method using real flight data from JFK to LAX.},
  copyright = {Copyright (c) 2019 Xinyu Zhao, Hao Yan, Jing Li, Yutian Pang, Yongming Liu},
  file = {/Users/hyan46/Zotero/storage/NH8KLL4K/854.html},
  journal = {Annual Conference of the PHM Society},
  keywords = {Anomaly Detection,Classification,Traffic},
  language = {en},
  number = {1}
}

@article{zhengAnatomicallyconstrained2020,
  title = {Anatomically-Constrained {{Deep Learning}} for {{Automating Dental CBCT Segmentation}} and {{Lesion Detection}}},
  author = {Zheng, Zhiyang and Yan, Hao and Setzer, Frank and Shi, Katherine and Mupparapu, Mel and Li, Jing},
  year = {2020},
  volume = {18},
  pages = {603--614},
  doi = {10.1109/TASE.2020.3025871},
  abstract = {Compared with the rapidly growing artificial intelligence (AI) research in other branches of healthcare, the pace of developing AI capacities in dental care is relatively slow. Dental care automation, especially the automated capability for dental cone beam computed tomography (CBCT) segmentation and lesion detection, is highly needed. CBCT is an important imaging modality that is experiencing ever-growing utilization in various dental specialties. However, little research has been done for segmenting different structures, restorative materials, and lesions using deep learning. This is due to multifold challenges such as content-rich oral cavity and significant within-label variation on each CBCT image as well as the inherent difficulty of obtaining many high-quality labeled images for training. On the other hand, oral-anatomical knowledge exists in dentistry, which shall be leveraged and integrated into the deep learning design. In this article, we propose a novel anatomically constrained Dense U-Net for integrating oral-anatomical knowledge with data-driven Dense U-Net. The proposed algorithm is formulated as a regularized or constrained optimization and solved using mean-field variational approximation to achieve computational efficiency. Mathematical encoding for transforming descriptive knowledge into a quantitative form is also proposed. Our experiment demonstrates that the proposed algorithm outperforms the standard Dense U-Net in both lesion detection accuracy and dice coefficient (DICE) indices in multilabel segmentation. Benefited from the integration with anatomical domain knowledge, our algorithm performs well with data from a small number of patients included in the training. Note to Practitioners \textemdash This article proposes a novel deep learning algorithm to enable the automated capability for cone beam computed tomography (CBCT) segmentation and lesion detection. Despite the growing adoption of CBCT in various dental specialties, such capability is curren...},
  copyright = {All rights reserved},
  file = {/Users/hyan46/Dropbox (ASU)/ZoteroData/Zheng et al_2020_Anatomically-constrained Deep Learning for Automating Dental CBCT Segmentation.pdf},
  journal = {IEEE Transactions on Automation Science and Engineering},
  keywords = {Deep Learning,Health,Image,Physics},
  number = {2}
}


